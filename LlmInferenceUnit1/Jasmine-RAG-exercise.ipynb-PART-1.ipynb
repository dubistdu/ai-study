{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d79705f-e5d2-49b0-9238-6f2abba09e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank_bm25 in ./myenv/lib/python3.13/site-packages (0.2.2)\n",
      "Requirement already satisfied: scikit-learn in ./myenv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in ./myenv/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.13/site-packages (from rank_bm25) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./myenv/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./myenv/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./myenv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in ./myenv/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./myenv/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./myenv/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (run these commands in a Colab cell)\n",
    "!pip install rank_bm25 scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e735c55-66cd-4e16-bec3-80732ba296fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jasmine.frantz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jasmine.frantz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 3: Modify the Document Corpus\n",
      "Original Corpus:\n",
      "  1. The quick brown fox jumps over the lazy dog.\n",
      "  2. Never jump over the lazy dog quickly.\n",
      "  3. A quick movement of the enemy will jeopardize six gunboats.\n",
      "  4. All questions asked by five watched experts amaze the judge.\n",
      "  5. The five boxing wizards jump quickly.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Would you like to add a new document? (yes/no):  yes\n",
      "Enter the new document text:  Where is the fox that jumped over the lazy pup\n",
      "Would you like to add another document? (yes/no):  yes\n",
      "Enter the new document text:  The five boxing idiots jump slowly.\n",
      "Would you like to add another document? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Corpus:\n",
      "  1. The quick brown fox jumps over the lazy dog.\n",
      "  2. Never jump over the lazy dog quickly.\n",
      "  3. A quick movement of the enemy will jeopardize six gunboats.\n",
      "  4. All questions asked by five watched experts amaze the judge.\n",
      "  5. The five boxing wizards jump quickly.\n",
      "  6. Where is the fox that jumped over the lazy pup\n",
      "  7. The five boxing idiots jump slowly.\n",
      "\n",
      "BM25 Scores for query 'quick jump':\n",
      "Score: 0.76  |  Document: The quick brown fox jumps over the lazy dog.\n",
      "Score: 0.27  |  Document: Never jump over the lazy dog quickly.\n",
      "Score: 0.72  |  Document: A quick movement of the enemy will jeopardize six gunboats.\n",
      "Score: 0.00  |  Document: All questions asked by five watched experts amaze the judge.\n",
      "Score: 0.28  |  Document: The five boxing wizards jump quickly.\n",
      "Score: 0.00  |  Document: Where is the fox that jumped over the lazy pup\n",
      "Score: 0.28  |  Document: The five boxing idiots jump slowly.\n",
      "\n",
      "TF-IDF (Cosine Similarity) Scores for query 'quick jump':\n",
      "Score: 0.30  |  Document: The quick brown fox jumps over the lazy dog.\n",
      "Score: 0.30  |  Document: Never jump over the lazy dog quickly.\n",
      "Score: 0.29  |  Document: A quick movement of the enemy will jeopardize six gunboats.\n",
      "Score: 0.00  |  Document: All questions asked by five watched experts amaze the judge.\n",
      "Score: 0.27  |  Document: The five boxing wizards jump quickly.\n",
      "Score: 0.00  |  Document: Where is the fox that jumped over the lazy pup\n",
      "Score: 0.26  |  Document: The five boxing idiots jump slowly.\n"
     ]
    }
   ],
   "source": [
    "# Import the word_tokenize function from NLTK for breaking text into tokens (words).\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Import BM25Okapi from the rank_bm25 package to implement the BM25 ranking algorithm,\n",
    "# which is widely used for information retrieval to score document relevance based on query terms.\n",
    "from rank_bm25 import BM25Okapi\n",
    "# Import TfidfVectorizer from scikit-learn to convert text documents into TF-IDF feature vectors.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Import cosine_similarity from scikit-learn to measure similarity between the query vector and document vectors.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Import the nltk library itself to manage downloading necessary data models.\n",
    "import nltk\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Download Required NLTK Data\n",
    "# -----------------------------------------------\n",
    "# The 'punkt' tokenizer model is required for the word_tokenize function to properly split sentences into tokens.\n",
    "# Downloading this data ensures that tokenization works correctly across various texts.\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "# -----------------------------------------------\n",
    "# Define the Original Document Corpus\n",
    "# -----------------------------------------------\n",
    "# A list of documents is defined here. In real applications, these might come from external files or databases.\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog quickly.\",\n",
    "    \"A quick movement of the enemy will jeopardize six gunboats.\",\n",
    "    \"All questions asked by five watched experts amaze the judge.\",\n",
    "    \"The five boxing wizards jump quickly.\"\n",
    "]\n",
    "\n",
    "def exercise3_modify_corpus():\n",
    "    \"\"\"\n",
    "    Exercise 3: Modify the Document Corpus\n",
    "\n",
    "    This interactive exercise demonstrates:\n",
    "      1. How to display and update a corpus of documents by allowing the user to add new documents.\n",
    "      2. How to preprocess the updated corpus (lowercase conversion and tokenization).\n",
    "      3. How to perform BM25-based document retrieval using a fixed query.\n",
    "      4. How to perform TF-IDF-based retrieval using cosine similarity with the same fixed query.\n",
    "\n",
    "    The exercise emphasizes the importance of text preprocessing and the effect it has on retrieval models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the header for this exercise.\n",
    "    print(\"Exercise 3: Modify the Document Corpus\")\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Display the Original Corpus\n",
    "    # -----------------------------------------------\n",
    "    print(\"Original Corpus:\")\n",
    "    # Enumerate through the original documents and print them with index numbers.\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"  {i+1}. {doc}\")\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Allow the User to Add New Documents to the Corpus\n",
    "    # -----------------------------------------------\n",
    "    # Ask the user if they want to add a new document. This makes the exercise interactive.\n",
    "    new_docs_input = input(\"\\nWould you like to add a new document? (yes/no): \")\n",
    "    # Initialize an empty list to store any new documents the user may provide.\n",
    "    new_docs = []\n",
    "    # Continue to ask the user for new documents until they type \"no\".\n",
    "    while new_docs_input.lower() == \"yes\":\n",
    "        # Prompt the user to enter the new document's text.\n",
    "        new_doc = input(\"Enter the new document text: \")\n",
    "        # Append the new document to the new_docs list.\n",
    "        new_docs.append(new_doc)\n",
    "        # Ask again if the user wants to add another document.\n",
    "        new_docs_input = input(\"Would you like to add another document? (yes/no): \")\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Update the Corpus with New Documents\n",
    "    # -----------------------------------------------\n",
    "    # Combine the original documents with any new documents added by the user.\n",
    "    updated_documents = documents + new_docs\n",
    "    print(\"\\nUpdated Corpus:\")\n",
    "    # Print the updated corpus with indices to show the complete list.\n",
    "    for i, doc in enumerate(updated_documents):\n",
    "        print(f\"  {i+1}. {doc}\")\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Preprocess the Updated Corpus\n",
    "    # -----------------------------------------------\n",
    "    # Preprocessing involves standardizing text, which in this case means converting to lowercase and tokenizing.\n",
    "    # Lowercasing is important because it reduces differences caused by case, ensuring that \"Dog\" and \"dog\" are treated the same.\n",
    "    # Tokenization splits text into individual words or tokens which are then used in the retrieval models.\n",
    "    tokenized_docs = [word_tokenize(doc.lower()) for doc in updated_documents]\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # BM25 Retrieval on the Updated Corpus\n",
    "    # -----------------------------------------------\n",
    "    # Initialize the BM25 model with the tokenized documents.\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    # Define a fixed query string to test retrieval. This could be modified or made interactive if desired.\n",
    "    query = \"quick jump\"\n",
    "    # Convert the query to lowercase and tokenize it to ensure consistency with the corpus preprocessing.\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    # Compute BM25 scores for each document. Higher scores indicate higher relevance to the query.\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    print(\"\\nBM25 Scores for query 'quick jump':\")\n",
    "    # Loop through the updated documents and their corresponding BM25 scores, printing each score.\n",
    "    for doc, score in zip(updated_documents, bm25_scores):\n",
    "        print(f\"Score: {score:.2f}  |  Document: {doc}\")\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # TF-IDF Retrieval using Cosine Similarity\n",
    "    # -----------------------------------------------\n",
    "    # Initialize a TfidfVectorizer from scikit-learn.\n",
    "    # The parameter stop_words='english' removes common English words (like \"the\", \"is\", etc.) that are unlikely to be informative.\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    # Fit the vectorizer on the updated documents and transform the documents into a TF-IDF matrix.\n",
    "    # The TF-IDF matrix represents each document as a vector of TF-IDF weights.\n",
    "    tfidf_matrix = vectorizer.fit_transform(updated_documents)\n",
    "    # Transform the query into the same TF-IDF vector space.\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    # Compute cosine similarity between the query vector and each document vector.\n",
    "    # Cosine similarity measures the cosine of the angle between two vectors, providing an indication of how similar they are.\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    print(\"\\nTF-IDF (Cosine Similarity) Scores for query 'quick jump':\")\n",
    "    # Loop through the updated documents and their corresponding cosine similarity scores, printing each score.\n",
    "    for doc, score in zip(updated_documents, cosine_similarities):\n",
    "        print(f\"Score: {score:.2f}  |  Document: {doc}\")\n",
    "\n",
    "# The following block checks if this script is being run directly,\n",
    "# ensuring that the exercise function is executed when the file is run as a standalone script.\n",
    "if __name__ == \"__main__\":\n",
    "    exercise3_modify_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4495ed0-e73e-44fd-92bb-70877b5ca92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b028e5-c296-4c4f-86e0-3e0b841da954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
