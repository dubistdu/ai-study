{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34eb2b9c-179a-413b-b2b8-9da8390ba0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is to be happy.\n",
      "I'm not sure if you're being sarcastic or not.\n",
      "I'm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Disable tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Automatically detect the correct device (MPS for Apple Silicon)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/opt-350m\"  # Replace with your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)  # Move model to MPS\n",
    "\n",
    "# Prepare input text and move input_ids to the same device\n",
    "input_text = \"The meaning of life is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")  # Convert input text to tensor\n",
    "# inputs is not a tensor but a BatchEncoding object.\n",
    "# To extract the actual tensor, you need to do:\n",
    "input_ids = inputs[\"input_ids\"].to(device)  # Extract `input_ids` and move to MPS\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids)\n",
    "\n",
    "# Decode and print output\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1110f412-24c1-4ba3-9169-e5d1eaff6b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  133, 3099,    9,  301,   16]], device='mps:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78ffa9f6-27d4-4f4a-8483-afc800c7ac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2,  133, 3099,    9,  301,   16]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The meaning of life is\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "24645560-3098-4867-80ec-e0b253b4ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87625adb-68d0-4347-943f-cca636c9cc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: modal in ./lib/python3.13/site-packages (0.73.116)\n",
      "Requirement already satisfied: aiohttp in ./lib/python3.13/site-packages (from modal) (3.11.14)\n",
      "Requirement already satisfied: certifi in ./lib/python3.13/site-packages (from modal) (2025.1.31)\n",
      "Requirement already satisfied: click>=8.1.0 in ./lib/python3.13/site-packages (from modal) (8.1.8)\n",
      "Requirement already satisfied: fastapi in ./lib/python3.13/site-packages (from modal) (0.115.11)\n",
      "Requirement already satisfied: grpclib==0.4.7 in ./lib/python3.13/site-packages (from modal) (0.4.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<6.0,>=3.19 in ./lib/python3.13/site-packages (from modal) (5.29.3)\n",
      "Requirement already satisfied: rich>=12.0.0 in ./lib/python3.13/site-packages (from modal) (13.9.4)\n",
      "Requirement already satisfied: synchronicity~=0.9.10 in ./lib/python3.13/site-packages (from modal) (0.9.11)\n",
      "Requirement already satisfied: toml in ./lib/python3.13/site-packages (from modal) (0.10.2)\n",
      "Requirement already satisfied: typer>=0.9 in ./lib/python3.13/site-packages (from modal) (0.15.2)\n",
      "Requirement already satisfied: types-certifi in ./lib/python3.13/site-packages (from modal) (2021.10.8.3)\n",
      "Requirement already satisfied: types-toml in ./lib/python3.13/site-packages (from modal) (0.10.8.20240310)\n",
      "Requirement already satisfied: watchfiles in ./lib/python3.13/site-packages (from modal) (1.0.4)\n",
      "Requirement already satisfied: typing_extensions~=4.6 in ./lib/python3.13/site-packages (from modal) (4.12.2)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in ./lib/python3.13/site-packages (from grpclib==0.4.7->modal) (4.2.0)\n",
      "Requirement already satisfied: multidict in ./lib/python3.13/site-packages (from grpclib==0.4.7->modal) (6.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./lib/python3.13/site-packages (from rich>=12.0.0->modal) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./lib/python3.13/site-packages (from rich>=12.0.0->modal) (2.19.1)\n",
      "Requirement already satisfied: sigtools>=4.0.1 in ./lib/python3.13/site-packages (from synchronicity~=0.9.10->modal) (4.0.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./lib/python3.13/site-packages (from typer>=0.9->modal) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./lib/python3.13/site-packages (from aiohttp->modal) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./lib/python3.13/site-packages (from aiohttp->modal) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./lib/python3.13/site-packages (from aiohttp->modal) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./lib/python3.13/site-packages (from aiohttp->modal) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./lib/python3.13/site-packages (from aiohttp->modal) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./lib/python3.13/site-packages (from aiohttp->modal) (1.18.3)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in ./lib/python3.13/site-packages (from fastapi->modal) (0.46.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in ./lib/python3.13/site-packages (from fastapi->modal) (2.10.6)\n",
      "Requirement already satisfied: anyio>=3.0.0 in ./lib/python3.13/site-packages (from watchfiles->modal) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./lib/python3.13/site-packages (from anyio>=3.0.0->watchfiles->modal) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./lib/python3.13/site-packages (from anyio>=3.0.0->watchfiles->modal) (1.3.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in ./lib/python3.13/site-packages (from h2<5,>=3.1.0->grpclib==0.4.7->modal) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in ./lib/python3.13/site-packages (from h2<5,>=3.1.0->grpclib==0.4.7->modal) (4.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->modal) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->modal) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->modal) (2.27.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf5472e3-5c8f-4c5d-bf8f-1c12b2f1bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Create a Modal image using Debian slim and install required dependencies\n",
    "# Original:\n",
    "# image = modal.Image.debian_slim().pip_install(\"fastapi[standard]\", \"transformers\")\n",
    "\n",
    "# Updated:\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"fastapi[standard]\",\n",
    "    \"transformers\",\n",
    "    \"torch>=2.0.0\",\n",
    "    \"accelerate\",\n",
    "    \"safetensors\"  # Optional but recommended for faster model loading\n",
    ")\n",
    "\n",
    "# Add local Python modules explicitly to avoid automounting warning\n",
    "image_with_source = image.add_local_python_source(\"_remote_module_non_scriptable\")\n",
    "\n",
    "# Initialize a Modal App with the custom image\n",
    "# Original:\n",
    "# app = modal.App(name=\"example-lifecycle-web\", image=image)\n",
    "\n",
    "# Updated:\n",
    "app = modal.App(name=\"example-lifecycle-web\", image=image_with_source)\n",
    "\n",
    "# Define a stub class for your model\n",
    "class MyModel:\n",
    "    def __init__(self):\n",
    "        # Load the tokenizer and model once during initialization\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\")\n",
    "    \n",
    "    def run_inference(self, input_text: str) -> str:\n",
    "        # Perform inference and return the result\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(**input_ids)\n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "# Define the web endpoint to accept requests and run inference\n",
    "@app.function()\n",
    "# Original:\n",
    "# @modal.web_endpoint()  # Expose this function via an HTTP endpoint\n",
    "\n",
    "# Updated:\n",
    "@modal.fastapi_endpoint()  # Updated from web_endpoint to fastapi_endpoint\n",
    "def hello(input_text: str) -> str:\n",
    "    # Initialize the model here, it will persist between calls\n",
    "    if not hasattr(hello, \"model_instance\"): # Only initialize on first call\n",
    "        hello.model_instance = MyModel()\n",
    "    result = hello.model_instance.run_inference(input_text)  # Run inference\n",
    "    return result\n",
    "\n",
    "# To run the web app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99be09-66d3-4d60-9c1f-a990025487a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60c505-73e3-47f3-b813-655d2647bc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f76b4-93a9-4cb9-88bd-30421988a1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
