{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34eb2b9c-179a-413b-b2b8-9da8390ba0d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Disable tokenizers parallelism warning\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Disable tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Automatically detect the correct device (MPS for Apple Silicon)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/opt-350m\"  # Replace with your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)  # Move model to MPS\n",
    "\n",
    "# Prepare input text and move input_ids to the same device\n",
    "input_text = \"The meaning of life is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")  # Convert input text to tensor\n",
    "# inputs is not a tensor but a BatchEncoding object.\n",
    "# To extract the actual tensor, you need to do:\n",
    "input_ids = inputs[\"input_ids\"].to(device)  # Extract `input_ids` and move to MPS\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids)\n",
    "\n",
    "# Decode and print output\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110f412-24c1-4ba3-9169-e5d1eaff6b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffa9f6-27d4-4f4a-8483-afc800c7ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"The meaning of life is\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24645560-3098-4867-80ec-e0b253b4ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87625adb-68d0-4347-943f-cca636c9cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5472e3-5c8f-4c5d-bf8f-1c12b2f1bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Create a Modal image using Debian slim and install required dependencies\n",
    "# Original:\n",
    "# image = modal.Image.debian_slim().pip_install(\"fastapi[standard]\", \"transformers\")\n",
    "\n",
    "# Updated:\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"fastapi[standard]\",\n",
    "    \"transformers\",\n",
    "    \"torch>=2.0.0\",\n",
    "    \"accelerate\",\n",
    "    \"safetensors\"  # Optional but recommended for faster model loading\n",
    ")\n",
    "\n",
    "# Add local Python modules explicitly to avoid automounting warning\n",
    "image_with_source = image.add_local_python_source(\"_remote_module_non_scriptable\")\n",
    "\n",
    "# Initialize a Modal App with the custom image\n",
    "# Original:\n",
    "# app = modal.App(name=\"example-lifecycle-web\", image=image)\n",
    "\n",
    "# Updated:\n",
    "app = modal.App(name=\"example-lifecycle-web\", image=image_with_source)\n",
    "\n",
    "# Define a stub class for your model\n",
    "class MyModel:\n",
    "    def __init__(self):\n",
    "        # Load the tokenizer and model once during initialization\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\")\n",
    "    \n",
    "    def run_inference(self, input_text: str) -> str:\n",
    "        # Perform inference and return the result\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(**input_ids)\n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "# Define the web endpoint to accept requests and run inference\n",
    "@app.function()\n",
    "# Original:\n",
    "# @modal.web_endpoint()  # Expose this function via an HTTP endpoint\n",
    "\n",
    "# Updated:\n",
    "@modal.fastapi_endpoint()  # Updated from web_endpoint to fastapi_endpoint\n",
    "def hello(input_text: str) -> str:\n",
    "    # Initialize the model here, it will persist between calls\n",
    "    if not hasattr(hello, \"model_instance\"): # Only initialize on first call\n",
    "        hello.model_instance = MyModel()\n",
    "    result = hello.model_instance.run_inference(input_text)  # Run inference\n",
    "    return result\n",
    "\n",
    "# To run the web app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99be09-66d3-4d60-9c1f-a990025487a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60c505-73e3-47f3-b813-655d2647bc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f76b4-93a9-4cb9-88bd-30421988a1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
